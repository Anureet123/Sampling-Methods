{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12dc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213bd827",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Creditcard_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0b7825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>575</td>\n",
       "      <td>-0.572263</td>\n",
       "      <td>0.731748</td>\n",
       "      <td>1.541254</td>\n",
       "      <td>0.150506</td>\n",
       "      <td>1.108974</td>\n",
       "      <td>0.372152</td>\n",
       "      <td>1.084879</td>\n",
       "      <td>-0.146329</td>\n",
       "      <td>-0.274447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143508</td>\n",
       "      <td>-0.107582</td>\n",
       "      <td>-0.418263</td>\n",
       "      <td>-0.731029</td>\n",
       "      <td>0.877525</td>\n",
       "      <td>-0.364150</td>\n",
       "      <td>-0.177509</td>\n",
       "      <td>-0.256545</td>\n",
       "      <td>26.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>579</td>\n",
       "      <td>-1.296845</td>\n",
       "      <td>-0.511605</td>\n",
       "      <td>2.404726</td>\n",
       "      <td>-0.310762</td>\n",
       "      <td>-0.319551</td>\n",
       "      <td>-0.542842</td>\n",
       "      <td>-0.173310</td>\n",
       "      <td>0.260423</td>\n",
       "      <td>-1.202688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071270</td>\n",
       "      <td>-0.161175</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.285390</td>\n",
       "      <td>0.281069</td>\n",
       "      <td>-0.370130</td>\n",
       "      <td>0.043410</td>\n",
       "      <td>0.092318</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>579</td>\n",
       "      <td>1.214170</td>\n",
       "      <td>0.210481</td>\n",
       "      <td>0.484651</td>\n",
       "      <td>0.479768</td>\n",
       "      <td>-0.261955</td>\n",
       "      <td>-0.527039</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>-0.106888</td>\n",
       "      <td>-0.037631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224292</td>\n",
       "      <td>-0.594609</td>\n",
       "      <td>0.159877</td>\n",
       "      <td>0.091873</td>\n",
       "      <td>0.140964</td>\n",
       "      <td>0.227406</td>\n",
       "      <td>-0.017389</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>580</td>\n",
       "      <td>1.267030</td>\n",
       "      <td>-0.071114</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>0.512683</td>\n",
       "      <td>0.242392</td>\n",
       "      <td>0.705212</td>\n",
       "      <td>-0.226582</td>\n",
       "      <td>0.109483</td>\n",
       "      <td>0.657565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164468</td>\n",
       "      <td>-0.177225</td>\n",
       "      <td>-0.222918</td>\n",
       "      <td>-1.245505</td>\n",
       "      <td>0.678360</td>\n",
       "      <td>0.525059</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>-0.003333</td>\n",
       "      <td>12.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>581</td>\n",
       "      <td>1.153758</td>\n",
       "      <td>0.132273</td>\n",
       "      <td>0.382969</td>\n",
       "      <td>1.405063</td>\n",
       "      <td>-0.224287</td>\n",
       "      <td>-0.197295</td>\n",
       "      <td>0.020653</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.412254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107809</td>\n",
       "      <td>-0.125231</td>\n",
       "      <td>-0.057041</td>\n",
       "      <td>0.073082</td>\n",
       "      <td>0.633977</td>\n",
       "      <td>-0.310685</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>0.015250</td>\n",
       "      <td>13.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0       0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1       0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "767   575 -0.572263  0.731748  1.541254  0.150506  1.108974  0.372152   \n",
       "768   579 -1.296845 -0.511605  2.404726 -0.310762 -0.319551 -0.542842   \n",
       "769   579  1.214170  0.210481  0.484651  0.479768 -0.261955 -0.527039   \n",
       "770   580  1.267030 -0.071114  0.037680  0.512683  0.242392  0.705212   \n",
       "771   581  1.153758  0.132273  0.382969  1.405063 -0.224287 -0.197295   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0    0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1   -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2    0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3    0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4    0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "767  1.084879 -0.146329 -0.274447  ... -0.143508 -0.107582 -0.418263   \n",
       "768 -0.173310  0.260423 -1.202688  ... -0.071270 -0.161175  0.088496   \n",
       "769  0.021782 -0.106888 -0.037631  ... -0.224292 -0.594609  0.159877   \n",
       "770 -0.226582  0.109483  0.657565  ... -0.164468 -0.177225 -0.222918   \n",
       "771  0.020653  0.029260  0.412254  ... -0.107809 -0.125231 -0.057041   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "0    0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1   -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
       "2   -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3   -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4    0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "..        ...       ...       ...       ...       ...     ...    ...  \n",
       "767 -0.731029  0.877525 -0.364150 -0.177509 -0.256545   26.72      0  \n",
       "768  0.285390  0.281069 -0.370130  0.043410  0.092318   80.00      0  \n",
       "769  0.091873  0.140964  0.227406 -0.017389  0.016030    5.98      0  \n",
       "770 -1.245505  0.678360  0.525059  0.002920 -0.003333   12.36      0  \n",
       "771  0.073082  0.633977 -0.310685  0.033590  0.015250   13.79      0  \n",
       "\n",
       "[772 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d1d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate fraudulent and non fraudulent data\n",
    "data_0 = data[data['Class'] == 0]\n",
    "data_1 = data[data['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2810134d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1      9\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "data.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78c61a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only 9000 0's sameple\n",
    "data_0 = data_0.sample(n=763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba67ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_38836\\1730244312.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data_1.append(data_0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    763\n",
       "1      9\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_1.append(data_0)\n",
    "\n",
    "data.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a001910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "data.to_csv('credit-card.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36de363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the no of 1's and 0's\n",
    "g = sns.countplot(data['Class'])\n",
    "g.set_xticklabels(['Not Fraud', 'Fraud'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc7610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 763)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# check length of 1's and 0's\n",
    "one = np.where(y==1)\n",
    "zero = np.where(y==0)\n",
    "len(one[0]), len(zero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849467e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4c9548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({0: 763, 1: 9})\n",
      "Resample dataset shape Counter({0: 9, 1: 9})\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
    "\n",
    "# fit predictor and target varialbe\n",
    "x_rus, y_rus = rus.fit_resample(x, y)\n",
    "\n",
    "print('original dataset shape:', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb62d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 763, 1: 9})\n",
      "Resample dataset shape Counter({1: 763, 0: 763})\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# fit predictor and target varaible\n",
    "x_ros, y_ros = ros.fit_resample(x, y)\n",
    "\n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2982fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROCAUC score: 0.9930555555555556\n",
      "Accuracy score: 0.9934640522875817\n",
      "F1 score: 0.9938650306748467\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_ros, y_ros, test_size =0.20, random_state=42)\n",
    "\n",
    "m = XGBClassifier()\n",
    "m.fit(x_train1, y_train1)\n",
    "predict_y = m.predict(x_test1)\n",
    "\n",
    "print('ROCAUC score:',roc_auc_score(y_test1, predict_y))\n",
    "print('Accuracy score:',accuracy_score(y_test1, predict_y))\n",
    "print('F1 score:',f1_score(y_test1, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24aaf710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169251a3",
   "metadata": {},
   "source": [
    "# 1.ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e81a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_ros, y_ros, test_size =0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb18a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Algorithms</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.990933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.989637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passive Aggressive Classifier</td>\n",
       "      <td>0.989637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification Algorithms     Score\n",
       "0                 KNN Classifier  0.988342\n",
       "1       Decision Tree Classifier  0.990933\n",
       "2            Logistic Regression  0.989637\n",
       "3  Passive Aggressive Classifier  0.989637\n",
       "4                            SVM  0.988342"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "logisticregression = LogisticRegression()\n",
    "knearestclassifier = KNeighborsClassifier()\n",
    "svm_classifier = SVC()\n",
    "bernoulli_naiveBayes = BernoulliNB()\n",
    "passiveAggressive = PassiveAggressiveClassifier()\n",
    "\n",
    "knearestclassifier.fit(x_train1, y_train1)\n",
    "decisiontree.fit(x_train1, y_train1)\n",
    "logisticregression.fit(x_train1, y_train1)\n",
    "passiveAggressive.fit(x_train1, y_train1)\n",
    "svm_classifier.fit(x_train1, y_train1)\n",
    "\n",
    "data1 = {\"Classification Algorithms\": [\"KNN Classifier\", \"Decision Tree Classifier\", \n",
    "                                       \"Logistic Regression\", \"Passive Aggressive Classifier\",\"SVM\"],\n",
    "      \"Score\": [knearestclassifier.score(x,y), decisiontree.score(x, y), \n",
    "                logisticregression.score(x, y), passiveAggressive.score(x,y),svm_classifier.score(x,y) ]}\n",
    "score = pd.DataFrame(data1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9ab80",
   "metadata": {},
   "source": [
    "# 2.K fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43db22f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.987080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.988362\n",
      "CART: 0.975391\n",
      "NB: 0.959824\n",
      "SVM: 0.988362\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=True,shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f\" % (name, cv_results.mean())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae691c1",
   "metadata": {},
   "source": [
    "# 3. Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5c7998a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>425</td>\n",
       "      <td>-0.367058</td>\n",
       "      <td>1.035851</td>\n",
       "      <td>1.108064</td>\n",
       "      <td>-0.065459</td>\n",
       "      <td>0.366306</td>\n",
       "      <td>-0.322658</td>\n",
       "      <td>0.596340</td>\n",
       "      <td>0.109672</td>\n",
       "      <td>-0.318671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262881</td>\n",
       "      <td>-0.671307</td>\n",
       "      <td>-0.022587</td>\n",
       "      <td>-0.347485</td>\n",
       "      <td>-0.197110</td>\n",
       "      <td>0.125083</td>\n",
       "      <td>0.258796</td>\n",
       "      <td>0.090689</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>530</td>\n",
       "      <td>1.445729</td>\n",
       "      <td>-0.992653</td>\n",
       "      <td>0.702959</td>\n",
       "      <td>-1.311315</td>\n",
       "      <td>-1.613625</td>\n",
       "      <td>-0.726591</td>\n",
       "      <td>-1.045431</td>\n",
       "      <td>-0.079224</td>\n",
       "      <td>-1.812438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409740</td>\n",
       "      <td>-0.819422</td>\n",
       "      <td>0.287770</td>\n",
       "      <td>0.322668</td>\n",
       "      <td>-0.007635</td>\n",
       "      <td>-0.469269</td>\n",
       "      <td>0.050763</td>\n",
       "      <td>0.027198</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>434</td>\n",
       "      <td>-0.679293</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>1.319394</td>\n",
       "      <td>1.249827</td>\n",
       "      <td>1.147786</td>\n",
       "      <td>-0.086534</td>\n",
       "      <td>1.001436</td>\n",
       "      <td>-0.039752</td>\n",
       "      <td>-1.374497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067521</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>-0.296954</td>\n",
       "      <td>-0.619850</td>\n",
       "      <td>0.282799</td>\n",
       "      <td>-0.059404</td>\n",
       "      <td>0.048695</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>25.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>380</td>\n",
       "      <td>1.158923</td>\n",
       "      <td>0.165064</td>\n",
       "      <td>0.507316</td>\n",
       "      <td>0.456052</td>\n",
       "      <td>-0.193600</td>\n",
       "      <td>-0.180777</td>\n",
       "      <td>-0.070487</td>\n",
       "      <td>0.046688</td>\n",
       "      <td>-0.302918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166915</td>\n",
       "      <td>-0.476946</td>\n",
       "      <td>0.139460</td>\n",
       "      <td>0.028225</td>\n",
       "      <td>0.130464</td>\n",
       "      <td>0.104225</td>\n",
       "      <td>-0.014329</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>297</td>\n",
       "      <td>1.168460</td>\n",
       "      <td>0.284169</td>\n",
       "      <td>0.427985</td>\n",
       "      <td>1.382027</td>\n",
       "      <td>-0.057586</td>\n",
       "      <td>-0.157170</td>\n",
       "      <td>0.132989</td>\n",
       "      <td>-0.070709</td>\n",
       "      <td>0.141838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091412</td>\n",
       "      <td>0.025132</td>\n",
       "      <td>-0.072434</td>\n",
       "      <td>0.121771</td>\n",
       "      <td>0.697131</td>\n",
       "      <td>-0.315949</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.017891</td>\n",
       "      <td>6.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>399</td>\n",
       "      <td>1.091666</td>\n",
       "      <td>0.068682</td>\n",
       "      <td>1.230334</td>\n",
       "      <td>2.554474</td>\n",
       "      <td>-0.454169</td>\n",
       "      <td>0.915781</td>\n",
       "      <td>-0.701016</td>\n",
       "      <td>0.396866</td>\n",
       "      <td>0.307571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098440</td>\n",
       "      <td>-0.178428</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>-0.504025</td>\n",
       "      <td>0.255974</td>\n",
       "      <td>0.022213</td>\n",
       "      <td>0.048714</td>\n",
       "      <td>0.027057</td>\n",
       "      <td>15.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>246</td>\n",
       "      <td>-1.069200</td>\n",
       "      <td>1.239963</td>\n",
       "      <td>0.545157</td>\n",
       "      <td>1.005354</td>\n",
       "      <td>-0.025696</td>\n",
       "      <td>-0.910673</td>\n",
       "      <td>0.422442</td>\n",
       "      <td>0.049283</td>\n",
       "      <td>-0.564601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>0.602990</td>\n",
       "      <td>0.132656</td>\n",
       "      <td>0.427113</td>\n",
       "      <td>-0.084030</td>\n",
       "      <td>-0.417194</td>\n",
       "      <td>-0.897885</td>\n",
       "      <td>-0.462042</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>241</td>\n",
       "      <td>-1.142321</td>\n",
       "      <td>0.626405</td>\n",
       "      <td>2.526917</td>\n",
       "      <td>2.827973</td>\n",
       "      <td>0.619263</td>\n",
       "      <td>0.897473</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>-0.060163</td>\n",
       "      <td>-0.813749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309746</td>\n",
       "      <td>-0.269173</td>\n",
       "      <td>0.177396</td>\n",
       "      <td>-0.019578</td>\n",
       "      <td>0.048651</td>\n",
       "      <td>0.068831</td>\n",
       "      <td>-0.246503</td>\n",
       "      <td>-0.230837</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>121</td>\n",
       "      <td>-0.868565</td>\n",
       "      <td>0.684978</td>\n",
       "      <td>2.013745</td>\n",
       "      <td>0.126390</td>\n",
       "      <td>-0.493158</td>\n",
       "      <td>-0.320478</td>\n",
       "      <td>0.373577</td>\n",
       "      <td>-0.080501</td>\n",
       "      <td>0.900231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.573843</td>\n",
       "      <td>-0.252014</td>\n",
       "      <td>0.431533</td>\n",
       "      <td>-0.137521</td>\n",
       "      <td>0.423221</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.044404</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "2       1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "567   425 -0.367058  1.035851  1.108064 -0.065459  0.366306 -0.322658   \n",
       "703   530  1.445729 -0.992653  0.702959 -1.311315 -1.613625 -0.726591   \n",
       "582   434 -0.679293  1.120837  1.319394  1.249827  1.147786 -0.086534   \n",
       "515   380  1.158923  0.165064  0.507316  0.456052 -0.193600 -0.180777   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "409   297  1.168460  0.284169  0.427985  1.382027 -0.057586 -0.157170   \n",
       "532   399  1.091666  0.068682  1.230334  2.554474 -0.454169  0.915781   \n",
       "336   246 -1.069200  1.239963  0.545157  1.005354 -0.025696 -0.910673   \n",
       "328   241 -1.142321  0.626405  2.526917  2.827973  0.619263  0.897473   \n",
       "184   121 -0.868565  0.684978  2.013745  0.126390 -0.493158 -0.320478   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "2    0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "567  0.596340  0.109672 -0.318671  ... -0.262881 -0.671307 -0.022587   \n",
       "703 -1.045431 -0.079224 -1.812438  ... -0.409740 -0.819422  0.287770   \n",
       "582  1.001436 -0.039752 -1.374497  ...  0.067521  0.030112 -0.296954   \n",
       "515 -0.070487  0.046688 -0.302918  ... -0.166915 -0.476946  0.139460   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "409  0.132989 -0.070709  0.141838  ... -0.091412  0.025132 -0.072434   \n",
       "532 -0.701016  0.396866  0.307571  ... -0.098440 -0.178428  0.007277   \n",
       "336  0.422442  0.049283 -0.564601  ...  0.146555  0.602990  0.132656   \n",
       "328  0.536278 -0.060163 -0.813749  ... -0.309746 -0.269173  0.177396   \n",
       "184  0.373577 -0.080501  0.900231  ...  0.027273  0.573843 -0.252014   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "2   -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "567 -0.347485 -0.197110  0.125083  0.258796  0.090689    3.59      0  \n",
       "703  0.322668 -0.007635 -0.469269  0.050763  0.027198   12.00      0  \n",
       "582 -0.619850  0.282799 -0.059404  0.048695  0.109200   25.03      0  \n",
       "515  0.028225  0.130464  0.104225 -0.014329  0.008026    9.99      0  \n",
       "..        ...       ...       ...       ...       ...     ...    ...  \n",
       "409  0.121771  0.697131 -0.315949  0.046610  0.017891    6.86      0  \n",
       "532 -0.504025  0.255974  0.022213  0.048714  0.027057   15.19      0  \n",
       "336  0.427113 -0.084030 -0.417194 -0.897885 -0.462042    2.98      0  \n",
       "328 -0.019578  0.048651  0.068831 -0.246503 -0.230837   10.62      0  \n",
       "184  0.431533 -0.137521  0.423221  0.180521  0.044404   29.99      0  \n",
       "\n",
       "[100 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(100, 31)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3. Random sampling\n",
    "df_sample_random = data.sample(n = 100, replace = False, random_state = None)\n",
    "display(df_sample_random)\n",
    "display(df_sample_random.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc7c4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_sample_random .iloc[:, :-1]\n",
    "y1 = df_sample_random .iloc[:, -1]\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size =0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fd0598c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Algorithms</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.963731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.965026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passive Aggressive Classifier</td>\n",
       "      <td>0.612694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification Algorithms     Score\n",
       "0                 KNN Classifier  0.988342\n",
       "1       Decision Tree Classifier  0.963731\n",
       "2            Logistic Regression  0.965026\n",
       "3  Passive Aggressive Classifier  0.612694\n",
       "4                            SVM  0.988342"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "logisticregression = LogisticRegression()\n",
    "knearestclassifier = KNeighborsClassifier()\n",
    "svm_classifier = SVC()\n",
    "bernoulli_naiveBayes = BernoulliNB()\n",
    "passiveAggressive = PassiveAggressiveClassifier()\n",
    "\n",
    "knearestclassifier.fit(x_train1, y_train1)\n",
    "decisiontree.fit(x_train1, y_train1)\n",
    "logisticregression.fit(x_train1, y_train1)\n",
    "passiveAggressive.fit(x_train1, y_train1)\n",
    "svm_classifier.fit(x_train1, y_train1)\n",
    "\n",
    "data1 = {\"Classification Algorithms\": [\"KNN Classifier\", \"Decision Tree Classifier\", \n",
    "                                       \"Logistic Regression\", \"Passive Aggressive Classifier\",\"SVM\"],\n",
    "      \"Score\": [knearestclassifier.score(x,y), decisiontree.score(x, y), \n",
    "                logisticregression.score(x, y), passiveAggressive.score(x,y),svm_classifier.score(x,y) ]}\n",
    "score = pd.DataFrame(data1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285e307",
   "metadata": {},
   "source": [
    "# 4. Systematic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e750948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>529</td>\n",
       "      <td>-2.000567</td>\n",
       "      <td>-2.495484</td>\n",
       "      <td>2.467149</td>\n",
       "      <td>1.140053</td>\n",
       "      <td>2.462010</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>-2.110183</td>\n",
       "      <td>0.788347</td>\n",
       "      <td>0.958809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422452</td>\n",
       "      <td>1.195394</td>\n",
       "      <td>0.297836</td>\n",
       "      <td>-0.857105</td>\n",
       "      <td>-0.219322</td>\n",
       "      <td>0.861019</td>\n",
       "      <td>-0.124622</td>\n",
       "      <td>-0.171060</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>380</td>\n",
       "      <td>-1.299837</td>\n",
       "      <td>0.881817</td>\n",
       "      <td>1.452842</td>\n",
       "      <td>-1.293698</td>\n",
       "      <td>-0.025105</td>\n",
       "      <td>-1.170103</td>\n",
       "      <td>0.861610</td>\n",
       "      <td>-0.193934</td>\n",
       "      <td>0.592001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272563</td>\n",
       "      <td>-0.360853</td>\n",
       "      <td>0.223911</td>\n",
       "      <td>0.598930</td>\n",
       "      <td>-0.397705</td>\n",
       "      <td>0.637141</td>\n",
       "      <td>0.234872</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>110</td>\n",
       "      <td>-0.591933</td>\n",
       "      <td>0.111273</td>\n",
       "      <td>0.699126</td>\n",
       "      <td>-1.536074</td>\n",
       "      <td>1.193208</td>\n",
       "      <td>0.648896</td>\n",
       "      <td>0.796706</td>\n",
       "      <td>0.016904</td>\n",
       "      <td>0.789664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205964</td>\n",
       "      <td>1.216195</td>\n",
       "      <td>0.093396</td>\n",
       "      <td>-0.900310</td>\n",
       "      <td>-0.423966</td>\n",
       "      <td>-0.607857</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>-0.076746</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>403</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>-2.631727</td>\n",
       "      <td>0.661576</td>\n",
       "      <td>-0.707254</td>\n",
       "      <td>-2.261000</td>\n",
       "      <td>0.360969</td>\n",
       "      <td>-1.069656</td>\n",
       "      <td>-0.053544</td>\n",
       "      <td>-0.833647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0.068132</td>\n",
       "      <td>-0.499373</td>\n",
       "      <td>-0.028743</td>\n",
       "      <td>0.460852</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.092994</td>\n",
       "      <td>411.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>140</td>\n",
       "      <td>-3.222745</td>\n",
       "      <td>-1.932776</td>\n",
       "      <td>2.923242</td>\n",
       "      <td>0.143574</td>\n",
       "      <td>0.948117</td>\n",
       "      <td>-1.076456</td>\n",
       "      <td>0.811226</td>\n",
       "      <td>-1.170683</td>\n",
       "      <td>1.363753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.657897</td>\n",
       "      <td>0.983985</td>\n",
       "      <td>1.304693</td>\n",
       "      <td>0.804414</td>\n",
       "      <td>0.686028</td>\n",
       "      <td>0.455321</td>\n",
       "      <td>-0.373342</td>\n",
       "      <td>-1.325164</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>204</td>\n",
       "      <td>-0.938226</td>\n",
       "      <td>0.104717</td>\n",
       "      <td>1.309642</td>\n",
       "      <td>-0.867537</td>\n",
       "      <td>0.450685</td>\n",
       "      <td>-1.437502</td>\n",
       "      <td>0.650604</td>\n",
       "      <td>0.028360</td>\n",
       "      <td>-0.588678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237086</td>\n",
       "      <td>-1.150829</td>\n",
       "      <td>0.262462</td>\n",
       "      <td>0.475206</td>\n",
       "      <td>-0.516790</td>\n",
       "      <td>0.426883</td>\n",
       "      <td>-0.058198</td>\n",
       "      <td>0.107996</td>\n",
       "      <td>39.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>575</td>\n",
       "      <td>-0.572263</td>\n",
       "      <td>0.731748</td>\n",
       "      <td>1.541254</td>\n",
       "      <td>0.150506</td>\n",
       "      <td>1.108974</td>\n",
       "      <td>0.372152</td>\n",
       "      <td>1.084879</td>\n",
       "      <td>-0.146329</td>\n",
       "      <td>-0.274447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143508</td>\n",
       "      <td>-0.107582</td>\n",
       "      <td>-0.418263</td>\n",
       "      <td>-0.731029</td>\n",
       "      <td>0.877525</td>\n",
       "      <td>-0.364150</td>\n",
       "      <td>-0.177509</td>\n",
       "      <td>-0.256545</td>\n",
       "      <td>26.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>160</td>\n",
       "      <td>-1.551482</td>\n",
       "      <td>-1.053346</td>\n",
       "      <td>0.762806</td>\n",
       "      <td>1.335129</td>\n",
       "      <td>-2.585523</td>\n",
       "      <td>1.942252</td>\n",
       "      <td>1.646687</td>\n",
       "      <td>0.641266</td>\n",
       "      <td>-0.649862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>1.254201</td>\n",
       "      <td>1.572315</td>\n",
       "      <td>-0.356254</td>\n",
       "      <td>-0.210821</td>\n",
       "      <td>-0.229607</td>\n",
       "      <td>-0.205431</td>\n",
       "      <td>0.039258</td>\n",
       "      <td>700.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "1       0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "541   406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "699   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
       "514   380 -1.299837  0.881817  1.452842 -1.293698 -0.025105 -1.170103   \n",
       "172   110 -0.591933  0.111273  0.699126 -1.536074  1.193208  0.648896   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "535   403  0.699599 -2.631727  0.661576 -0.707254 -2.261000  0.360969   \n",
       "211   140 -3.222745 -1.932776  2.923242  0.143574  0.948117 -1.076456   \n",
       "284   204 -0.938226  0.104717  1.309642 -0.867537  0.450685 -1.437502   \n",
       "767   575 -0.572263  0.731748  1.541254  0.150506  1.108974  0.372152   \n",
       "240   160 -1.551482 -1.053346  0.762806  1.335129 -2.585523  1.942252   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "1   -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "541 -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "699 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
       "514  0.861610 -0.193934  0.592001  ... -0.272563 -0.360853  0.223911   \n",
       "172  0.796706  0.016904  0.789664  ...  0.205964  1.216195  0.093396   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "535 -1.069656 -0.053544 -0.833647  ...  0.049882  0.068132 -0.499373   \n",
       "211  0.811226 -1.170683  1.363753  ... -0.657897  0.983985  1.304693   \n",
       "284  0.650604  0.028360 -0.588678  ... -0.237086 -1.150829  0.262462   \n",
       "767  1.084879 -0.146329 -0.274447  ... -0.143508 -0.107582 -0.418263   \n",
       "240  1.646687  0.641266 -0.649862  ...  0.867609  1.254201  1.572315   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "1   -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
       "541  0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "699 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
       "514  0.598930 -0.397705  0.637141  0.234872  0.021379    0.00      0  \n",
       "172 -0.900310 -0.423966 -0.607857  0.017705 -0.076746   13.95      0  \n",
       "..        ...       ...       ...       ...       ...     ...    ...  \n",
       "535 -0.028743  0.460852 -0.003403  0.020449  0.092994  411.88      0  \n",
       "211  0.804414  0.686028  0.455321 -0.373342 -1.325164   11.00      0  \n",
       "284  0.475206 -0.516790  0.426883 -0.058198  0.107996   39.94      0  \n",
       "767 -0.731029  0.877525 -0.364150 -0.177509 -0.256545   26.72      0  \n",
       "240 -0.356254 -0.210821 -0.229607 -0.205431  0.039258  700.00      0  \n",
       "\n",
       "[258 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define systematic sampling function\n",
    "def systematic_sampling(df, step):\n",
    " \n",
    "    indexes = np.arange(0, len(df), step=step)\n",
    "    systematic_sample = df.iloc[indexes]\n",
    "    return systematic_sample\n",
    " \n",
    "#Obtain a systematic sample and save it in a new variable\n",
    "systematic_sample = systematic_sampling(data, 3)\n",
    " \n",
    "# View sampled data frame\n",
    "display(systematic_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ade8a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = systematic_sample .iloc[:, :-1]\n",
    "y1 = systematic_sample.iloc[:, -1]\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size =0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e2e8a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Algorithms</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.989637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passive Aggressive Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification Algorithms     Score\n",
       "0                 KNN Classifier  0.988342\n",
       "1       Decision Tree Classifier  0.988342\n",
       "2            Logistic Regression  0.989637\n",
       "3  Passive Aggressive Classifier  0.988342\n",
       "4                            SVM  0.988342"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisiontree = DecisionTreeClassifier()\n",
    "logisticregression = LogisticRegression()\n",
    "knearestclassifier = KNeighborsClassifier()\n",
    "svm_classifier = SVC()\n",
    "bernoulli_naiveBayes = BernoulliNB()\n",
    "passiveAggressive = PassiveAggressiveClassifier()\n",
    "\n",
    "knearestclassifier.fit(x_train1, y_train1)\n",
    "decisiontree.fit(x_train1, y_train1)\n",
    "logisticregression.fit(x_train1, y_train1)\n",
    "passiveAggressive.fit(x_train1, y_train1)\n",
    "svm_classifier.fit(x_train1, y_train1)\n",
    "\n",
    "data1 = {\"Classification Algorithms\": [\"KNN Classifier\", \"Decision Tree Classifier\", \n",
    "                                       \"Logistic Regression\", \"Passive Aggressive Classifier\",\"SVM\"],\n",
    "      \"Score\": [knearestclassifier.score(x,y), decisiontree.score(x, y), \n",
    "                logisticregression.score(x, y), passiveAggressive.score(x,y),svm_classifier.score(x,y) ]}\n",
    "score = pd.DataFrame(data1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fa641",
   "metadata": {},
   "source": [
    "# 5. Cluster Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44889c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "1       0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "3       1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
      "5       2 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
      "4       2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
      "6       4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
      "..    ...       ...       ...       ...       ...       ...       ...   \n",
      "764   574 -1.062129 -0.618574  0.615388 -3.335834  0.746649 -0.540531   \n",
      "766   574  1.257719  0.364739  0.306923  0.690638 -0.357792 -1.067481   \n",
      "763   574 -0.402057  0.584300  2.474227  0.929684  0.014314  0.297490   \n",
      "769   579  1.214170  0.210481  0.484651  0.479768 -0.261955 -0.527039   \n",
      "770   580  1.267030 -0.071114  0.037680  0.512683  0.242392  0.705212   \n",
      "\n",
      "           V7        V8        V9  ...       V21       V22       V23  \\\n",
      "1   -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
      "3    0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
      "5    0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398   \n",
      "4    0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
      "6   -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "764  0.705932  0.032525  1.334181  ... -0.114269 -0.661122  0.136250   \n",
      "766  0.094272 -0.210300  0.014455  ... -0.286856 -0.820658  0.127663   \n",
      "763  0.715195 -0.257153  0.593868  ... -0.072812  0.445733 -0.245103   \n",
      "769  0.021782 -0.106888 -0.037631  ... -0.224292 -0.594609  0.159877   \n",
      "770 -0.226582  0.109483  0.657565  ... -0.164468 -0.177225 -0.222918   \n",
      "\n",
      "          V24       V25       V26       V27       V28  Amount  Class  \n",
      "1   -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
      "3   -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
      "5   -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
      "4    0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
      "6   -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99      0  \n",
      "..        ...       ...       ...       ...       ...     ...    ...  \n",
      "764 -1.377245  0.263820 -1.115516  0.079213  0.116638  131.37      0  \n",
      "766  0.343128  0.221120  0.094391 -0.022189  0.030944    1.29      1  \n",
      "763  0.421234  0.049280 -0.388323 -0.329333 -0.386747   12.00      0  \n",
      "769  0.091873  0.140964  0.227406 -0.017389  0.016030    5.98      0  \n",
      "770 -1.245505  0.678360  0.525059  0.002920 -0.003333   12.36      0  \n",
      "\n",
      "[500 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "samples = data.sample(500).sort_values(by='Time')\n",
    " \n",
    "# show samples\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d29dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = samples.iloc[:, :-1]\n",
    "y1 = samples .iloc[:, -1]\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size =0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4dd6f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Algorithms</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.981865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.987047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passive Aggressive Classifier</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.988342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classification Algorithms     Score\n",
       "0                 KNN Classifier  0.988342\n",
       "1       Decision Tree Classifier  0.981865\n",
       "2            Logistic Regression  0.987047\n",
       "3  Passive Aggressive Classifier  0.988342\n",
       "4                            SVM  0.988342"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisiontree = DecisionTreeClassifier()\n",
    "logisticregression = LogisticRegression()\n",
    "knearestclassifier = KNeighborsClassifier()\n",
    "svm_classifier = SVC()\n",
    "bernoulli_naiveBayes = BernoulliNB()\n",
    "passiveAggressive = PassiveAggressiveClassifier()\n",
    "\n",
    "knearestclassifier.fit(x_train1, y_train1)\n",
    "decisiontree.fit(x_train1, y_train1)\n",
    "logisticregression.fit(x_train1, y_train1)\n",
    "passiveAggressive.fit(x_train1, y_train1)\n",
    "svm_classifier.fit(x_train1, y_train1)\n",
    "\n",
    "data1 = {\"Classification Algorithms\": [\"KNN Classifier\", \"Decision Tree Classifier\",\"Logistic Regression\"\n",
    "                                       , \"Passive Aggressive Classifier\",\"SVM\"],\n",
    "      \"Score\": [knearestclassifier.score(x,y), decisiontree.score(x, y), \n",
    "                logisticregression.score(x, y), passiveAggressive.score(x,y),svm_classifier.score(x,y) ]}\n",
    "score = pd.DataFrame(data1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226e63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
